# -*- coding: utf-8 -*-
"""cars_pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J7Rg0Fw-TkJpD56xo-p6TnC6a3LFYXbf
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import torch.backends.cudnn as cudnn
import numpy as np
import torchvision
from torchvision import datasets, models, transforms
import matplotlib.pyplot as plt
import time
import os
from PIL import Image
from tempfile import TemporaryDirectory

cudnn.benchmark = True
plt.ion()   # interactive mode

# Data augmentation and normalization for training
# Just normalization for validation
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'test': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

data_dir = 'cars_data_7'
image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),
                                          data_transforms[x])
                  for x in ['train','test']}
dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,
                                             shuffle=True, num_workers=4)
              for x in ['train','test']}
dataset_sizes = {x: len(image_datasets[x]) for x in ['train','test']}
class_names = image_datasets['train'].classes

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

def imshow(inp, title=None):
    """Display image for Tensor."""
    inp = inp.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.imshow(inp)
    if title is not None:
        plt.title(title)
    plt.pause(0.001)  # pause a bit so that plots are updated


# Get a batch of training data
inputs, classes = next(iter(dataloaders['train']))

# Make a grid from batch
out = torchvision.utils.make_grid(inputs)

imshow(out, title=[class_names[x] for x in classes])

def train_model(model, criterion, optimizer, scheduler, num_epochs=25):
    since = time.time()

    # Create a temporary directory to save training checkpoints
    with TemporaryDirectory() as tempdir:
        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')

        torch.save(model.state_dict(), best_model_params_path)
        best_acc = 0.0

        # Initialize lists to store losses and accuracies
        train_losses = []
        train_accs = []
        test_losses = []
        test_accs = []


        for epoch in range(num_epochs):
            print(f'Epoch {epoch}/{num_epochs - 1}')
            print('-' * 10)

            # Each epoch has a training and validation phase
            for phase in ['train', 'test']:
                if phase == 'train':
                    model.train()  # Set model to training mode
                else:
                    model.eval()   # Set model to evaluate mode

                running_loss = 0.0
                running_corrects = 0

                # Iterate over data.
                for inputs, labels in dataloaders[phase]:
                    inputs = inputs.to(device)
                    labels = labels.to(device)

                    # zero the parameter gradients
                    optimizer.zero_grad()

                    # forward
                    # track history if only in train
                    with torch.set_grad_enabled(phase == 'train'):
                        outputs = model(inputs)
                        _, preds = torch.max(outputs, 1)
                        loss = criterion(outputs, labels)

                        # backward + optimize only if in training phase
                        if phase == 'train':
                            loss.backward()
                            optimizer.step()

                    # statistics
                    running_loss += loss.item() * inputs.size(0)
                    running_corrects += torch.sum(preds == labels.data)
                if phase == 'train':
                    scheduler.step()

                epoch_loss = running_loss / dataset_sizes[phase]
                epoch_acc = running_corrects.double() / dataset_sizes[phase]

                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

                # Store losses and accuracies in respective lists
                if phase == 'train':
                    train_losses.append(epoch_loss)
                    train_accs.append(epoch_acc)
                else:
                    test_losses.append(epoch_loss)
                    test_accs.append(epoch_acc)

                # deep copy the model
                if phase == 'test' and epoch_acc > best_acc:
                    best_acc = epoch_acc
                    torch.save(model.state_dict(), best_model_params_path)

            print()

        time_elapsed = time.time() - since
        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
        print(f'Best val Acc: {best_acc:4f}')

        # load best model weights
        model.load_state_dict(torch.load(best_model_params_path))
    return model, train_losses, train_accs, test_losses, test_accs

# Plotting loss and accuracies for training and validation
def plot_losses_accuracies(train_losses, train_accs, test_losses, test_accs):
    """
    Plots the training and validation losses and accuracies.

    Args:
        train_losses (list): List of training losses per epoch.
        train_accs (list): List of training accuracies per epoch.
        test_losses (list): List of validation losses per epoch.
        test_accs (list): List of validation accuracies per epoch.
    """

    import matplotlib.pyplot as plt

    epochs = range(len(train_accs))  # Create epochs list for plotting

    # Plot accuracy
    plt.plot(epochs, train_accs, 'r', label='Training accuracy')
    plt.plot(epochs, test_accs, 'b', label='Validation accuracy')
    plt.title('Training and validation accuracy')
    plt.legend()
    plt.figure()

    # Plot loss
    plt.plot(epochs, train_losses, 'r', label='Training Loss')
    plt.plot(epochs, test_losses, 'b', label='Validation Loss')
    plt.title('Training and validation loss')
    plt.legend()

    plt.show()

def visualize_model(model, num_images=10):
    was_training = model.training
    model.eval()
    images_so_far = 0
    fig = plt.figure()

    with torch.no_grad():
        for i, (inputs, labels) in enumerate(dataloaders['test']):
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            for j in range(inputs.size()[0]):
                images_so_far += 1
                ax = plt.subplot(num_images//2, 2, images_so_far)
                ax.axis('off')
                ax.set_title(f'predicted: {class_names[preds[j]]}')
                imshow(inputs.cpu().data[j])

                if images_so_far == num_images:
                    model.train(mode=was_training)
                    return
        model.train(mode=was_training)

import torch.nn.functional as F
import torch
from torcheval.metrics import MulticlassF1Score, MulticlassPrecision, TopKMultilabelAccuracy

def evaluate_model(model, dataloader, criterion, num_classes=10, average="macro", k=2, criteria="exact_match"):
    """
    Evaluates the model on the given dataloader and calculates multi-class precision, F1 score,
    and top-k multilabel accuracy metrics.

    Args:
        model: PyTorch model.
        dataloader: Dataloader containing validation or test data.
        criterion: Loss function used during training.
        num_classes (int, optional): Number of classes for the metric calculations.
                                     If None, it will be inferred from the ground truth labels. Defaults to None.
        average (str, optional): How to average scores across classes.
                                     Options: 'macro' (default), 'weighted', 'micro', or None (per-class scores).
        k (int, optional): Number of top classes to consider for TopKMultilabelAccuracy. Defaults to 2.
        criteria (str, optional): Criterion to use for TopKMultilabelAccuracy.
                                  Options: 'exact', 'hamming', 'overlap', or 'contain'. Defaults to 'exact'.

    Returns:
        tuple: (f1_score, precision_score, top_k_accuracy, average_loss)
    """

    model.eval()  # Set model to evaluation mode

    total_loss = 0  # Initialize to accumulate loss over all batches

    # Create metrics with appropriate averaging
    f1_metric = MulticlassF1Score(num_classes=num_classes, average=average)
    precision_metric = MulticlassPrecision(num_classes=num_classes, average=average)
    top_k_accuracy_metric = TopKMultilabelAccuracy(k=k, criteria=criteria)

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)

            # Calculate loss for the current batch
            batch_loss = criterion(outputs, labels).item()
            total_loss += batch_loss

            if torch.nn.functional.softmax(model(inputs), dim=-1).shape == outputs.shape:  # Ensure softmax alignment
                f1_metric.update(outputs, labels)
                precision_metric.update(outputs, labels)
                labels_onehot = F.one_hot(labels, num_classes=num_classes)
                top_k_accuracy_metric.update(outputs, labels_onehot)

    # Calculate average loss over all batches
    average_loss = total_loss / len(dataloader)

    # Compute multi-class metrics
    f1_score = f1_metric.compute()
    precision_score = precision_metric.compute()
    top_k_accuracy = top_k_accuracy_metric.compute()

    return f1_score, precision_score, top_k_accuracy, average_loss

# CHANGE MODEL AS PER CONVENIENCE
odel_ft = models.resnet18(weights='IMAGENET1K_V1')
num_ftrs = model_ft.fc.in_features
# Here the size of each output sample is set to 2.
# Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``.
model_ft.fc = nn.Linear(num_ftrs, 10)

model_ft = model_ft.to(device)

criterion = nn.CrossEntropyLoss()

# Observe that all parameters are being optimized
optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)

# Decay LR by a factor of 0.1 every 7 epochs
exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)

model_resnet18, train_losses, train_accs, test_losses, test_accs = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,
                       num_epochs=10)

# Saving the model weights
model_weights_path = "./model_weights_resnet18.pt"
torch.save(model_resnet18.state_dict(), model_weights_path)

# Plots for loss and acc
plot_losses_accuracies(train_losses, train_accs, test_losses, test_accs)

#from sklearn.metrics import classification_report,confusion_matrix
#predictions = model_resnet18.predict(dataloaders['test'])
#predictions=np.argmax(predictions,axis=-1)
#dict_cls = dataloaders['test'].class_indices
#print(classification_report(dataloaders['test'].labels, predictions, target_names = list(dict_cls.keys())))
f1_score, precision_score, top_k_accuracy, average_loss = evaluate_model(model_resnet18, dataloaders['test'], criterion)

print(f1_score, precision_score, top_k_accuracy)

visualize_model(model_resnet18)

